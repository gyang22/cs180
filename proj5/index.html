<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Image Gallery</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            background-color: #f4f4f4;
        }
        h1 {
            text-align: center;
            color: #333;
        }
        .gallery {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 20px;
        }
        .gallery-item {
            width: 400px;
            border: 1px solid #ccc;
            border-radius: 5px;
            overflow: hidden;
            background-color: #fff;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        .gallery-item-small {
            width: 300px;
            border: 1px solid #ccc;
            border-radius: 5px;
            overflow: hidden;
            background-color: #fff;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        .gallery-item img {
            width: 100%;
            height: auto;
        }
        .gallery-item p {
            padding: 15px;
            text-align: center;
            color: #555;
        }
    </style>
</head>
<body>

<h1>Project 5: Fun With Diffusion Models</h1>
<h1>Part A: The Power of Diffusion Models</h1>

<p>In this project, we explore the implementation and application of diffusion models in denoising images, which can be used to sample "novel" images from pure noise and generate cool artwork/visual anagrams.</p>

<h2>Testing Prompts</h2>
<p>We use a pretrained model called DeepFloyd to save compute, which is originally intended for text-to-image use. We get around this by using a "null" prompt of "a high quality photo" which serves as a general prompt, but here we also display some of the pretrained text to image capabilities:</p>
<div class="gallery">
    <div class="gallery-item">
        <img src="proj5_data/snowy.png">
        <p>Prompt: an oil painting of a snowy mountain village</p>
    </div>
    <div class="gallery-item">
        <img src="proj5_data/hat.png">
        <p>Prompt: a man wearing a hat</p>
    </div>
    <div class="gallery-item">
        <img src="proj5_data/rocket.png">
        <p>Prompt: a rocket ship</p>
    </div>
</div>

<h2>Implementing the Forward Process</h2>
<p>First, we need a way to add noise to clean images at different levels. We implement a function that adds scaled Gaussian noise to images, and show the results at levels 0 (no noise), 250, 500, and 750.</p>
<div class="gallery">
    <div class="gallery-item">
        <img src="proj5_data/test_im.jpg">
        <p>0</p>
    </div>
    <div class="gallery-item">
        <img src="proj5_data/test_im_250.jpg">
        <p>250</p>
    </div>
    <div class="gallery-item">
        <img src="proj5_data/test_im_500.jpg">
        <p>500</p>
    </div>
    <div class="gallery-item">
        <img src="proj5_data/test_im_750.jpg">
        <p>750</p>
    </div>
</div>

<h2>Classical Denoising</h2>
<p>We can first try simple methods for denoising, such as Gaussian blur filtering to see the results. Using a kernel size of 5 and sigma of 2, we obtain the lackluster results of the blurred/denoised images:</p>
<div class="gallery">
    <div class="gallery-item">
        <img src="proj5_data/test_im_250.jpg">
        <img src="proj5_data/blur_test_250.jpg">
        <p>250</p>
    </div>
    <div class="gallery-item">
        <img src="proj5_data/test_im_500.jpg">
        <img src="proj5_data/blur_test_500.jpg">
        <p>500</p>
    </div>
    <div class="gallery-item">
        <img src="proj5_data/test_im_750.jpg">
        <img src="proj5_data/blur_test_750.jpg">
        <p>750</p>
    </div>
</div>
<p>With lower noise levels, the blur denoising works alright, but as the noise increases it becomes more obvious, and we also sacrifice image clarity in this way.</p>

<h2>One-Step Denoising</h2>
<p>We can improve this performance by using a diffusion model to predict the noise in the image. In this section, we use a pretrained model for simplicity, to which we can feed the noisy image and the timestep/scale of the noise t, and get a noise estimate as an output, then scale this noise by the inverse factor we multiplied by in the original forward step. The results for model-predicted denoising is shown for the three levels:</p>
<div class="gallery">
    <div class="gallery-item">
        <img src="proj5_data/test_im_250.jpg">
        <img src="proj5_data/one_step_250.png">
        <p>250</p>
    </div>
    <div class="gallery-item">
        <img src="proj5_data/test_im_500.jpg">
        <img src="proj5_data/one_step_500.png">
        <p>500</p>
    </div>
    <div class="gallery-item">
        <img src="proj5_data/test_im_750.jpg">
        <img src="proj5_data/one_step_750.png">
        <p>750</p>
    </div>
</div>

<h2>Iterative Denoising</h2>
<p>In order to improve our performance, we can instead iteratively denoise the image by iterating on our timesteps, and slowly subtracting the predicted noise at each level. The formula involves the noise scaling factor we originally noised the image with, then we attempt to predict the next cleaner step of the image by getting the model's estimate, then subtracting a scaled version of it from the blurry image, then add back some additional noise to aid in our "search" for the optimal image. Once we reach the final timestep, we have found our clean image, or our best estimate of it.</p>
<p>We create our timesteps as a linear interpolation between 990 and 0, with a step size down of 30 each time, and show the denoising process every fifth step.</p>
<div class="gallery">
    <div class="gallery-item">
        <img src="proj5_data/iter_1.png">
        <p>t = 690</p>
    </div>
    <div class="gallery-item">
        <img src="proj5_data/iter_2.png">
        <p>t = 540</p>
    </div>
    <div class="gallery-item">
        <img src="proj5_data/iter_3.png">
        <p>t = 390</p>
    </div>
    <div class="gallery-item">
        <img src="proj5_data/iter_4.png">
        <p>t = 240</p>
    </div>
    <div class="gallery-item">
        <img src="proj5_data/iter_5.png">
        <p>t = 90</p>
    </div>
    <div class="gallery-item">
        <img src="proj5_data/im_clean.png">
        <p>t = 0 (denoised)</p>
    </div>
    <div class="gallery-item">
        <img src="proj5_data/im_noisy.png">
        <p>Fully noisy</p>
    </div>
    <div class="gallery-item">
        <img src="proj5_data/test_im.jpg">
        <p>Original</p>
    </div>
    <div class="gallery-item">
        <img src="proj5_data/im_clean_one_step.png">
        <p>Denoised (one-step)</p>
    </div>
    <div class="gallery-item">
        <img src="proj5_data/im_blur_filter.png">
        <p>Denoised (blur filter)</p>
    </div>
</div>

<h2>Diffusion Model Sampling</h2>
<p>Now that we have a iterative denoising framework in place, we can begin to sample/generate novel images from the model. We accomplish this by feeding an image of complete noise into the denoising process, so that the image generated is completely dependent on the random path that the model chooses to the image domain. By using the null prompt "a high quality photo" we generate 5 random images below:</p>
<div class="gallery">
    <div class="gallery-item">
        <img src="proj5_data/one_1.png">
    </div>
    <div class="gallery-item">
        <img src="proj5_data/one_2.png">
    </div>
    <div class="gallery-item">
        <img src="proj5_data/one_3.png">
    </div>
    <div class="gallery-item">
        <img src="proj5_data/one_4.png">
    </div>
    <div class="gallery-item">
        <img src="proj5_data/one_5.png">
    </div>
</div>

<h2>Classifier-Free Guidance (CFG)</h2>
<p>We can improve the quality/sensibility of our generated images by using classifier-free guidance, which is also generating an unconditional noise estimate alongside the conditioned one, then combining them with a scale of 7.0 on their difference. The scale factor controls the strength of this guidance. We get the following images using CFG:</p>
<div class="gallery">
    <div class="gallery-item">
        <img src="proj5_data/cfg_1.png">
    </div>
    <div class="gallery-item">
        <img src="proj5_data/cfg_2.png">
    </div>
    <div class="gallery-item">
        <img src="proj5_data/cfg_3.png">
    </div>
    <div class="gallery-item">
        <img src="proj5_data/cfg_4.png">
    </div>
    <div class="gallery-item">
        <img src="proj5_data/cfg_5.png">
    </div>
</div>

<h2>Image-to-image Translation</h2>
<p>We can edit existing images by applying noise to them, then using CFG to denoise them back to the original image domain. As we continue to increase the amount of added noise, the resulting image becomes further from the original, leaving room for "creative" interpretation.</p>


<h1>Part B: Diffusion Models from Scratch</h1>
<p>In this next part, we implement diffusion models from scratch using the U-net neural network architecture. We train our model on the MNIST handwritten digits dataset, then apply the same denoising and sampling processing to generate digits from pure noise.</p>
<h1>Training a Single-Step Denoising UNet</h1>
<p>Our first goal was to build a one-step denoiser like in part A.</p>
<h2>Implementing the UNet</h2>
<p>Using PyTorch, we implement the general structure of the U-net as described below:</p>
<div style="display: flex;
justify-content: center;
align-items: center;">
    <img src="proj5_data/unet_diagram.png" style="width: 70%; height: auto; margin: auto;">
</div>

<h2>Using the UNet to Train a Denoiser</h2>
<p>We first need a way to apply noise to clean images, in this case standard normal noise scaled by a constant sigma. The noise levels are shown below for varying sigma:</p>
<div style="display: flex;
justify-content: center;
align-items: center;">
    <img src="proj5_data/blur_levels.png" style="width: 40%; height: auto; margin: auto;">
</div>
<h2>Training</h2>
<p>In order to train the model, we take clean images of digits and apply Gaussian noise to them with sigma = 0.5, then run gradient descent on the mean squared error between the clean image and predicted output. Below is the training loss curve over 5 epochs.</p>
<div style="display: flex;
justify-content: center;
align-items: center;">
    <img src="proj5_data/single_step_loss.png" style="width: 40%; height: auto; margin: auto;">
</div>
<p>Testing our model on test set images also made noisy with sigma 0.5 yields the following results for training after epoch 1:</p>
<div style="display: flex;
justify-content: center;
align-items: center;">
    <img src="proj5_data/one_step_results_1.png" style="width: 30%; height: auto; margin: auto;">
</div>
<p>and the following results after epoch 5:</p>
<div style="display: flex;
justify-content: center;
align-items: center;">
    <img src="proj5_data/one_step_results_5.png" style="width: 30%; height: auto; margin: auto;">
</div>
<h2>Out-of-Distribution Testing</h2>
<p>Further, to test on sigma/noise values other than 0.5, we also did out-of-distribution testing, noting the lower quality for higher noise levels it was not trained on:</p>
<div style="display: flex;
justify-content: center;
align-items: center;">
    <img src="proj5_data/one_step_out.png" style="width: 80%; height: auto; margin: auto;">
</div>

<h1>Training a Diffusion Model</h1>
<p>Now, instead of just directly predicting the clean image from the noisy, we attempt to predict the noise in the image instead, much like the DeepFloyd model from part A. We create a list of scaling factors and their inverses, alpha and beta, then iteratively subtract small amounts of noise from the image like in part A.</p>

<h2>Adding Time Conditioning to UNet</h2>
<p>In order to additionally condition the model output on the timestep t, we need to inject the timestep into the network using a fully connected block, and then sum into the intermediate steps. The timestep is scaled to be in [0, 1] before injecting to better be understoof by the model.</p>
<div style="display: flex;
justify-content: center;
align-items: center;">
    <img src="proj5_data/fc_diagram.png" style="width: 70%; height: auto; margin: auto;">
</div>

<h2>Training the UNet</h2>
<p>Training the model follows a similar process to before, but this time we generate a random t for each image and apply that level of noise to the image prior to input, so that the model is exposed to all levels of noise and time. The training loss curve (in log scale) is shown below:</p>
<div style="display: flex;
justify-content: center;
align-items: center;">
    <img src="proj5_data/time_loss.png" style="width: 40%; height: auto; margin: auto;">
</div>

<h2>Sampling from the UNet</h2>
<p>Finally, we can sample from the model like in part A, to generate digits from pure noise. We start with a pure noise image, then iteratively denoise with decreasing timesteps, until we eventually reach timestep 0, a theoretically clean digit.</p>
<p>Sampling results after epoch 5:</p>
<div style="display: flex;
justify-content: center;
align-items: center;">
    <img src="proj5_data/time_5_results.png" style="width: 60%; height: auto; margin: auto;">
</div>
<p>Results after epoch 20:</p>
<div style="display: flex;
justify-content: center;
align-items: center;">
    <img src="proj5_data/time_20_results.png" style="width: 60%; height: auto; margin: auto;">
</div>
<p>There are clearly some problems with this approach, as many of the generated digits do not make sense. We fix this in the next section.</p>

<h2>Adding Class-Conditioning to UNet</h2>
<p>We can add the option to additionally condition our model on the class (aka digit 0-9) of the image. This allows us to more specifically sample digits from each class, by passing in a one-hot vector representing the digit. We can also choose to unconditionally generate by passing in a vector of all zeros. We can account for this null condition by randomly setting some class vectors to 0 during training, with probability 10%. The training loss curve in log scale is shown below:</p>
<div style="display: flex;
justify-content: center;
align-items: center;">
    <img src="proj5_data/class_loss.png" style="width: 40%; height: auto; margin: auto;">
</div>

<h2>Sampling from the Class-Conditioned UNet</h2>
<p>Much like in part A, we need classifer-free guidance to assist our results, so we generate a null result alongside a class-conditioned result then combine them with a scale factor of 5.0. Otherwise the sampling process is much the same as the other iterative denoising cycles.</p>
<p>Sampling after epoch 5:</p>
<div style="display: flex;
justify-content: center;
align-items: center;">
    <img src="proj5_data/class_results_5.png" style="width: 60%; height: auto; margin: auto;">
</div>
<p>After epoch 20:</p>
<div style="display: flex;
justify-content: center;
align-items: center;">
    <img src="proj5_data/class_results_20.png" style="width: 60%; height: auto; margin: auto;">
</div>
</body>
</html>
