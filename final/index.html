<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Image Gallery</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            background-color: #f4f4f4;
        }
        h1 {
            text-align: center;
            color: #333;
        }
        .gallery {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 20px;
        }
        .gallery-item {
            width: 400px;
            border: 1px solid #ccc;
            border-radius: 5px;
            overflow: hidden;
            background-color: #fff;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        .gallery-item-small {
            width: 300px;
            border: 1px solid #ccc;
            border-radius: 5px;
            overflow: hidden;
            background-color: #fff;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        .gallery-item img {
            width: 100%;
            height: auto;
        }
        .gallery-item p {
            padding: 15px;
            text-align: center;
            color: #555;
        }
        .image-row {
            display: flex;
            justify-content: space-between;
            align-items: center;
            gap: 20px;
            padding: 10px;
        }

        .image-container {
            text-align: center;
            flex: 1;
        }

        .image-container img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 0 auto;
        }

        .image-caption {
            margin-top: 5px;
            font-size: 14px;
            color: #555;
        }
    </style>
</head>
<body>

<h1>NeRFs: Neural Radiance Fields</h1>

<p>In this project, we designed a neural radiance field, a neural network designed to generate novel views of 3D objects given multi-view training data of the object. Using this model, we can create seamless videos of objects despite not having complete information about their geometry.</p>

<h2>Neural Fields on 2D Images</h2>
<p>First, we start by training a more simple network on 2D images, with the goal being to predict the RGB pixel values of a set of coordinates. The model architecture used is a multi-layer perceptron of 4 layers with 256 width, using RELU activation and sigmoid normalization at the end. The diagram is shown below:</p>
<div style="display: flex;
justify-content: center;
align-items: center;">
    <img src="data/mlp_img.jpg" style="width: 50%; height: auto; margin: auto;">
</div>
<p>Before inputting data to the model, we must first positionally encode it to expand its dimensionality, by appending to it a series of sines and cosines as below, where L is the highest level. For the positional data in this project, we used L=10.</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
    <mi>P</mi>
    <mi>E</mi>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
    <mo>=</mo>
    <mo fence="false" stretchy="false">{</mo>
    <mi>x</mi>
    <mo>,</mo>
    <mi>s</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mo stretchy="false">(</mo>
    <msup>
      <mn>2</mn>
      <mn>0</mn>
    </msup>
    <mi>&#x3C0;</mi>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
    <mo>,</mo>
    <mi>c</mi>
    <mi>o</mi>
    <mi>s</mi>
    <mo stretchy="false">(</mo>
    <msup>
      <mn>2</mn>
      <mn>0</mn>
    </msup>
    <mi>&#x3C0;</mi>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
    <mo>,</mo>
    <mi>s</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mo stretchy="false">(</mo>
    <msup>
      <mn>2</mn>
      <mn>1</mn>
    </msup>
    <mi>&#x3C0;</mi>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
    <mo>,</mo>
    <mi>c</mi>
    <mi>o</mi>
    <mi>s</mi>
    <mo stretchy="false">(</mo>
    <msup>
      <mn>2</mn>
      <mn>1</mn>
    </msup>
    <mi>&#x3C0;</mi>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
    <mo>,</mo>
    <mo>.</mo>
    <mo>.</mo>
    <mo>.</mo>
    <mo>,</mo>
    <mi>s</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mo stretchy="false">(</mo>
    <msup>
      <mn>2</mn>
      <mrow data-mjx-texclass="ORD">
        <mi>L</mi>
        <mo>&#x2212;</mo>
        <mn>1</mn>
      </mrow>
    </msup>
    <mi>&#x3C0;</mi>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
    <mo>,</mo>
    <mi>c</mi>
    <mi>o</mi>
    <mi>s</mi>
    <mo stretchy="false">(</mo>
    <msup>
      <mn>2</mn>
      <mrow data-mjx-texclass="ORD">
        <mi>L</mi>
        <mo>&#x2212;</mo>
        <mn>1</mn>
      </mrow>
    </msup>
    <mi>&#x3C0;</mi>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
    <mo fence="false" stretchy="false">}</mo>
  </math>
<p>For the dataloader, we sample uniformly with a batch size of 10k from the pixels in the image, normalizing the locations and colors to [0, 1].</p>
<p>We use mean squared error (MSE) for the loss function and the Adam optimizer with a 1e-2 learning rate, training the model for 1000 iterations. We also graph the peak signal-to-noise ratio (PSNR) as a measure of reconstruction quality. The model is trained on this image of a fox:</p>
<div class="image-row">
    <div class="image-container">
        <img src="data/fox.jpg">
        <div class="image-caption">Original Image</div>
    </div>
    <div class="image-container">
        <img src="data/fox_loss.png">
        <div class="image-caption">MSE Loss</div>
    </div>
    <div class="image-container">
        <img src="data/fox_psnr.png">
        <div class="image-caption">PSNR</div>
    </div>
</div>
<div class="image-row">
    <div class="image-container">
        <img src="data/fox1.png">
        <div class="image-caption">Iteration 1</div>
    </div>
    <div class="image-container">
        <img src="data/fox2.png">
        <div class="image-caption">Iteration 250</div>
    </div>
    <div class="image-container">
        <img src="data/fox3.png">
        <div class="image-caption">Iteration 500</div>
    </div>
    <div class="image-container">
        <img src="data/fox4.png">
        <div class="image-caption">Iteration 750</div>
    </div>
    <div class="image-container">
        <img src="data/fox5.png">
        <div class="image-caption">Iteration 1000</div>
    </div>
</div>
<p>Additionally, if we pick a slower learning rate of 5e-4, we can see the more incomplete intermediate inferences of the model as it trains more clearly, and it eventually reaches nearly a similar level of accuracy:</p>
<div class="image-row">
    <div class="image-container">
        <img src="data/fox_loss_slow.png">
        <div class="image-caption">MSE Loss</div>
    </div>
    <div class="image-container">
        <img src="data/fox_psnr_slow.png">
        <div class="image-caption">PSNR</div>
    </div>
</div>
<div class="image-row">
    <div class="image-container">
        <img src="data/fox1slow.png">
        <div class="image-caption">Iteration 1</div>
    </div>
    <div class="image-container">
        <img src="data/fox2slow.png">
        <div class="image-caption">Iteration 250</div>
    </div>
    <div class="image-container">
        <img src="data/fox3slow.png">
        <div class="image-caption">Iteration 500</div>
    </div>
    <div class="image-container">
        <img src="data/fox4slow.png">
        <div class="image-caption">Iteration 750</div>
    </div>
    <div class="image-container">
        <img src="data/fox5slow.png">
        <div class="image-caption">Iteration 1000</div>
    </div>
</div>
<p>We also try a more drastic change: keeping the learning rate slower and the iterations the same, as well as all other hyperparameters, we change the hidden layer width to be 64 while also quadrupling the number of hidden layers. The total amount of neurons is around the same, but the performance is noticeably worse, maybe requiring longer to train on a deeper model:</p>
<div class="image-row">
    <div class="image-container">
        <img src="data/fox_loss_deep.png">
        <div class="image-caption">MSE Loss</div>
    </div>
    <div class="image-container">
        <img src="data/fox_psnr_deep.png">
        <div class="image-caption">PSNR</div>
    </div>
</div>
<div class="image-row">
    <div class="image-container">
        <img src="data/fox1deep.png">
        <div class="image-caption">Iteration 1</div>
    </div>
    <div class="image-container">
        <img src="data/fox2deep.png">
        <div class="image-caption">Iteration 250</div>
    </div>
    <div class="image-container">
        <img src="data/fox3deep.png">
        <div class="image-caption">Iteration 500</div>
    </div>
    <div class="image-container">
        <img src="data/fox4deep.png">
        <div class="image-caption">Iteration 750</div>
    </div>
    <div class="image-container">
        <img src="data/fox5deep.png">
        <div class="image-caption">Iteration 1000</div>
    </div>
</div>
<p>We can also try running the same hyperparameters again on a picture of a lighthouse instead, with the original set of hyperparameters:</p>
<div class="image-row">
    <div class="image-container">
        <img src="data/lighthouse.jpg">
        <div class="image-caption">Original Image</div>
    </div>
    <div class="image-container">
        <img src="data/light_loss.png">
        <div class="image-caption">MSE Loss</div>
    </div>
    <div class="image-container">
        <img src="data/light_psnr.png">
        <div class="image-caption">PSNR</div>
    </div>
</div>
<div class="image-row">
    <div class="image-container">
        <img src="data/light1.png">
        <div class="image-caption">Iteration 1</div>
    </div>
    <div class="image-container">
        <img src="data/light2.png">
        <div class="image-caption">Iteration 250</div>
    </div>
    <div class="image-container">
        <img src="data/light3.png">
        <div class="image-caption">Iteration 500</div>
    </div>
    <div class="image-container">
        <img src="data/light4.png">
        <div class="image-caption">Iteration 750</div>
    </div>
    <div class="image-container">
        <img src="data/light5.png">
        <div class="image-caption">Iteration 1000</div>
    </div>
</div>
<p>Then using the slower learning rate of 5e-4:</p>
<div class="image-row">
    <div class="image-container">
        <img src="data/light_loss_slow.png">
        <div class="image-caption">MSE Loss</div>
    </div>
    <div class="image-container">
        <img src="data/light_psnr_slow.png">
        <div class="image-caption">PSNR</div>
    </div>
</div>
<div class="image-row">
    <div class="image-container">
        <img src="data/light1slow.png">
        <div class="image-caption">Iteration 1</div>
    </div>
    <div class="image-container">
        <img src="data/light2slow.png">
        <div class="image-caption">Iteration 250</div>
    </div>
    <div class="image-container">
        <img src="data/light3slow.png">
        <div class="image-caption">Iteration 500</div>
    </div>
    <div class="image-container">
        <img src="data/light4slow.png">
        <div class="image-caption">Iteration 750</div>
    </div>
    <div class="image-container">
        <img src="data/light5slow.png">
        <div class="image-caption">Iteration 1000</div>
    </div>
</div>
<p>And trying the deeper, thinner architecture again:</p>
<div class="image-row">
    <div class="image-container">
        <img src="data/light_loss_deep.png">
        <div class="image-caption">MSE Loss</div>
    </div>
    <div class="image-container">
        <img src="data/light_psnr_deep.png">
        <div class="image-caption">PSNR</div>
    </div>
</div>
<div class="image-row">
    <div class="image-container">
        <img src="data/light1deep.png">
        <div class="image-caption">Iteration 1</div>
    </div>
    <div class="image-container">
        <img src="data/light2deep.png">
        <div class="image-caption">Iteration 250</div>
    </div>
    <div class="image-container">
        <img src="data/light3deep.png">
        <div class="image-caption">Iteration 500</div>
    </div>
    <div class="image-container">
        <img src="data/light4deep.png">
        <div class="image-caption">Iteration 750</div>
    </div>
    <div class="image-container">
        <img src="data/light5deep.png">
        <div class="image-caption">Iteration 1000</div>
    </div>
</div>
<p>The deeper architecture appears to learn the color much later than the shape, maybe implying that the wider layers are able to better embed the color map of the image.</p>

<h2>Fitting a Neural Radiance Field from Multi-view Images</h2>
<p>The previous task may have seemed more trivial, but we now pivot to training a model to represent a 3D space through inverse rendering from multi-view calibrated images. The cameras and images are taken from the original <a href="https://www.matthewtancik.com/nerf">NeRF</a> paper, and the below image shows the setup of cameras around the object with training cameras in black, validation in red, and testing in green:</p>
<div style="display: flex;
justify-content: center;
align-items: center;">
    <img src="data/cameras.png" style="width: 50%; height: auto; margin: auto;">
</div>
<h3>Creating Rays from Cameras</h3>
<p>Since our input is images taken from calibrated cameras, we must transform between points in the camera space and world space. We have a 3x3 rotation matrix R and a translation vector t that combine to create the world-to-camera (w2c) matrix:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
    <mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
      <mtr>
        <mtd>
          <mrow data-mjx-texclass="INNER">
            <mo data-mjx-texclass="OPEN">[</mo>
            <mtable columnalign="center" columnspacing="1em" rowspacing="4pt">
              <mtr>
                <mtd>
                  <msub>
                    <mi>x</mi>
                    <mi>c</mi>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>y</mi>
                    <mi>c</mi>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>z</mi>
                    <mi>c</mi>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mn>1</mn>
                </mtd>
              </mtr>
            </mtable>
            <mo data-mjx-texclass="CLOSE">]</mo>
          </mrow>
          <mo>=</mo>
          <mrow data-mjx-texclass="INNER">
            <mo data-mjx-texclass="OPEN">[</mo>
            <mtable columnalign="center" columnspacing="1em" rowspacing="4pt">
              <mtr>
                <mtd>
                  <msub>
                    <mrow data-mjx-texclass="ORD">
                      <mi mathvariant="bold">R</mi>
                    </mrow>
                    <mrow data-mjx-texclass="ORD">
                      <mn>3</mn>
                      <mo>&#xD7;</mo>
                      <mn>3</mn>
                    </mrow>
                  </msub>
                </mtd>
                <mtd>
                  <mrow data-mjx-texclass="ORD">
                    <mi mathvariant="bold">t</mi>
                  </mrow>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mrow data-mjx-texclass="ORD">
                      <mn mathvariant="bold">0</mn>
                    </mrow>
                    <mrow data-mjx-texclass="ORD">
                      <mn>1</mn>
                      <mo>&#xD7;</mo>
                      <mn>3</mn>
                    </mrow>
                  </msub>
                </mtd>
                <mtd>
                  <mn>1</mn>
                </mtd>
              </mtr>
            </mtable>
            <mo data-mjx-texclass="CLOSE">]</mo>
          </mrow>
          <mrow data-mjx-texclass="INNER">
            <mo data-mjx-texclass="OPEN">[</mo>
            <mtable columnalign="center" columnspacing="1em" rowspacing="4pt">
              <mtr>
                <mtd>
                  <msub>
                    <mi>x</mi>
                    <mi>w</mi>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>y</mi>
                    <mi>w</mi>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>z</mi>
                    <mi>w</mi>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mn>1</mn>
                </mtd>
              </mtr>
            </mtable>
            <mo data-mjx-texclass="CLOSE">]</mo>
          </mrow>
        </mtd>
      </mtr>
    </mtable>
  </math>
<p>Thus, to transform in the other direction, from camera to world, we take the inverse of the w2c matrix (c2w) and matmul with our points.</p>
<p>Next, given a pinhole camera model with focal length f_x, f_y, and principal point (o_x = image_width / 2, o_y = image_height / 2), we create its intrinsic matrix K:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
    <mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
      <mtr>
        <mtd>
          <mrow data-mjx-texclass="ORD">
            <mi mathvariant="bold">K</mi>
          </mrow>
          <mo>=</mo>
          <mrow data-mjx-texclass="INNER">
            <mo data-mjx-texclass="OPEN">[</mo>
            <mtable columnalign="center" columnspacing="1em" rowspacing="4pt">
              <mtr>
                <mtd>
                  <msub>
                    <mi>f</mi>
                    <mi>x</mi>
                  </msub>
                </mtd>
                <mtd>
                  <mn>0</mn>
                </mtd>
                <mtd>
                  <msub>
                    <mi>o</mi>
                    <mi>x</mi>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mn>0</mn>
                </mtd>
                <mtd>
                  <msub>
                    <mi>f</mi>
                    <mi>y</mi>
                  </msub>
                </mtd>
                <mtd>
                  <msub>
                    <mi>o</mi>
                    <mi>y</mi>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mn>0</mn>
                </mtd>
                <mtd>
                  <mn>0</mn>
                </mtd>
                <mtd>
                  <mn>1</mn>
                </mtd>
              </mtr>
            </mtable>
            <mo data-mjx-texclass="CLOSE">]</mo>
          </mrow>
        </mtd>
      </mtr>
    </mtable>
  </math>
<p>and use it to project 3D points to 2D points on the image plane.</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
    <mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
      <mtr>
        <mtd>
          <mi>s</mi>
          <mrow data-mjx-texclass="INNER">
            <mo data-mjx-texclass="OPEN">[</mo>
            <mtable columnalign="center" columnspacing="1em" rowspacing="4pt">
              <mtr>
                <mtd>
                  <mi>u</mi>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mi>v</mi>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mn>1</mn>
                </mtd>
              </mtr>
            </mtable>
            <mo data-mjx-texclass="CLOSE">]</mo>
          </mrow>
          <mo>=</mo>
          <mrow data-mjx-texclass="ORD">
            <mi mathvariant="bold">K</mi>
          </mrow>
          <mrow data-mjx-texclass="INNER">
            <mo data-mjx-texclass="OPEN">[</mo>
            <mtable columnalign="center" columnspacing="1em" rowspacing="4pt">
              <mtr>
                <mtd>
                  <msub>
                    <mi>x</mi>
                    <mi>c</mi>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>y</mi>
                    <mi>c</mi>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>z</mi>
                    <mi>c</mi>
                  </msub>
                </mtd>
              </mtr>
            </mtable>
            <mo data-mjx-texclass="CLOSE">]</mo>
          </mrow>
        </mtd>
      </mtr>
    </mtable>
  </math>
<p>Once we are given the 2D location pixels, we can define a ray given as an origin and a direction, for each pixel in the image. The origins are just the camera that captured them, and the direction can be computed by taking a point on the ray at depth 1 and subtracting and normalizing the origin from its world coordinate.</p>

<h3>Sampling</h3>
<p>With the definitions in place, we can implement the sampling process. We flatten all training set image pixels and choose N uniformly, then use the above equations to transform these pixels into rays. We then sample uniformly along these rays to get world points, adding small noise pertubations to avoid overfitting. We choose 32 points along each ray, with 10k pixels sampled in each iteration.</p>

<h3>Visualizing Dataloader</h3>
<p>Using provided starter code, we can visualize the sampling process with the rays and points from each camera:</p>
<div style="display: flex;
justify-content: center;
align-items: center;">
    <img src="data/sample_render.png" style="width: 50%; height: auto; margin: auto;">
</div>

<h3>Neural Radiance Field Architecture</h3>
<p>We use a modified multi-layer perceptron structure with a feedforward injection of the input to help the model remember, as well as splitting the hidden layers into simultaneous density and color prediction. The model takes as input the 3D coordinates of the world points as well as the ray direction they affect.</p>
<div style="display: flex;
justify-content: center;
align-items: center;">
    <img src="data/mlp_nerf.png" style="width: 50%; height: auto; margin: auto;">
</div>

<h3>Volume Rendering</h3>
<p>Given the color and density predictions at each point along a ray, we can compute an estimate of the contributions each point gives to the overall volume of the object. We use a discrete approximation instead of the full integral, since we only sample at 32 points, and sum over the points:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
    <mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
      <mtr>
        <mtd>
          <mrow data-mjx-texclass="ORD">
            <mover>
              <mi>C</mi>
              <mo stretchy="false">^</mo>
            </mover>
          </mrow>
          <mo stretchy="false">(</mo>
          <mrow data-mjx-texclass="ORD">
            <mi mathvariant="bold">r</mi>
          </mrow>
          <mo stretchy="false">)</mo>
          <mo>=</mo>
          <munderover>
            <mo data-mjx-texclass="OP">&#x2211;</mo>
            <mrow data-mjx-texclass="ORD">
              <mi>i</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mi>N</mi>
          </munderover>
          <msub>
            <mi>T</mi>
            <mi>i</mi>
          </msub>
          <mrow data-mjx-texclass="INNER">
            <mo data-mjx-texclass="OPEN">(</mo>
            <mn>1</mn>
            <mo>&#x2212;</mo>
            <mi>exp</mi>
            <mo data-mjx-texclass="NONE">&#x2061;</mo>
            <mrow data-mjx-texclass="INNER">
              <mo data-mjx-texclass="OPEN">(</mo>
              <mo>&#x2212;</mo>
              <msub>
                <mi>&#x3C3;</mi>
                <mi>i</mi>
              </msub>
              <msub>
                <mi>&#x3B4;</mi>
                <mi>i</mi>
              </msub>
              <mo data-mjx-texclass="CLOSE">)</mo>
            </mrow>
            <mo data-mjx-texclass="CLOSE">)</mo>
          </mrow>
          <msub>
            <mrow data-mjx-texclass="ORD">
              <mi mathvariant="bold">c</mi>
            </mrow>
            <mi>i</mi>
          </msub>
          <mo>,</mo>
          <mtext>&#xA0;where&#xA0;</mtext>
          <msub>
            <mi>T</mi>
            <mi>i</mi>
          </msub>
          <mo>=</mo>
          <mi>exp</mi>
          <mo data-mjx-texclass="NONE">&#x2061;</mo>
          <mrow data-mjx-texclass="INNER">
            <mo data-mjx-texclass="OPEN">(</mo>
            <mo>&#x2212;</mo>
            <munderover>
              <mo data-mjx-texclass="OP">&#x2211;</mo>
              <mrow data-mjx-texclass="ORD">
                <mi>j</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mrow data-mjx-texclass="ORD">
                <mi>i</mi>
                <mo>&#x2212;</mo>
                <mn>1</mn>
              </mrow>
            </munderover>
            <msub>
              <mi>&#x3C3;</mi>
              <mi>j</mi>
            </msub>
            <msub>
              <mi>&#x3B4;</mi>
              <mi>j</mi>
            </msub>
            <mo data-mjx-texclass="CLOSE">)</mo>
          </mrow>
        </mtd>
      </mtr>
    </mtable>
  </math>
<p>Training and Results</p>
<p>With the setup in place, we begin training the model over 2500 iterations with MSE loss and Adam optimizer at a 5e-4 learning rate. We sample with a 10k batch size and manage to reach 25 PSNR after full training. We visualize the PSNR curve on a separate validation dataset and also show the progression of the model on a novel test camera. We also show another example of the sampling process with 32 points per ray, with 100 rays.</p>

<p>The resulting video generated from novel-view test cameras:</p>
<div style="display: flex;
justify-content: center;
align-items: center;">
    <img src="data/lego_gif.gif" style="width: 50%; height: auto; margin: auto;">
</div>

<h3>Bells and Whistles</h3>
<p>We can modify the volume rendering equation to change the resulting background color of the images. Rather than having no contribution, we add an additional term to the sum consisting of the transmittance after the last sample, or the light that escapes the volume, scaled by the background color of our choosing. We use dark blue in the example below:</p>
<div style="display: flex;
justify-content: center;
align-items: center;">
    <img src="data/blue_lego_gif.gif" style="width: 50%; height: auto; margin: auto;">
</div>
</body>
</html>
